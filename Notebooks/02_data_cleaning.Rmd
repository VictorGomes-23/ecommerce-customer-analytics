---
title: "Data Cleaning - Online Retail Dataset"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    highlight: tango
---

# Project Overview

This notebook performs comprehensive data cleaning on the Online Retail Dataset. Based on our exploration analysis, we identified several data quality issues that need to be addressed before proceeding with customer analytics.

**Key Cleaning Tasks:**
- Convert data types to proper formats
- Create derived fields for analysis
- Handle missing values systematically
- Address returns and cancellations
- Remove invalid prices and duplicates
- Create customer-level summary metrics

**Input:** `data/raw/OnlineRetail.csv`  
**Output:** Three cleaned datasets in `data/processed/`

---

# Setup and Configuration

```{r setup, include=FALSE}
# Configure code chunk defaults
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = 'center'
)
```

```{r load-libraries}
# Data manipulation and cleaning
library(tidyverse)      # Core data manipulation
library(lubridate)      # Date and time handling
library(here)           # Relative file paths
library(janitor)        # Data cleaning utilities

# Analysis and reporting
library(skimr)          # Data summaries
library(scales)         # Formatting
library(knitr)          # Table formatting
library(kableExtra)     # Enhanced tables
```

**What we just did:** Loaded all necessary packages for data cleaning and validation. The `janitor` package provides helpful cleaning functions.

---

# Load Raw Data

```{r load-raw-data}
# Load the raw dataset from CSV
retail_raw <- read_csv(here("data", "raw", "Online_Retail.csv"))

# Store original dimensions for tracking
original_rows <- nrow(retail_raw)
original_cols <- ncol(retail_raw)

# Display confirmation
cat("Raw data loaded successfully!\n")
cat("Original dimensions:", original_rows, "rows x", original_cols, "columns\n")
```

**What we found:** Raw data loaded. We've saved the original dimensions to track how cleaning affects the dataset size.

```{r preview-raw}
# Quick preview of raw data structure
glimpse(retail_raw)
```

**Initial structure:** Before cleaning, we can see the data types and initial format of each column.

---

# Step 1: Data Type Conversions

Now we'll convert each column to its proper data type for analysis.

```{r convert-data-types}
# Create a working copy and convert data types
retail_cleaned <- retail_raw %>%
  mutate(
    # Convert InvoiceDate from character to datetime (M/D/YYYY H:M format)
    InvoiceDate = mdy_hm(InvoiceDate),
    
    # Convert CustomerID to character (IDs should be categorical, not numeric)
    CustomerID = as.character(CustomerID),
    
    # Ensure Quantity is integer
    Quantity = as.integer(Quantity),
    
    # Ensure UnitPrice is numeric
    UnitPrice = as.numeric(UnitPrice),
    
    # Clean all character columns by trimming whitespace
    InvoiceNo = str_trim(InvoiceNo),
    StockCode = str_trim(StockCode),
    Description = str_trim(Description),
    Country = str_trim(Country)
  )

# Verify conversions
cat("Data type conversions completed.\n")
cat("InvoiceDate class:", class(retail_cleaned$InvoiceDate), "\n")
cat("CustomerID class:", class(retail_cleaned$CustomerID), "\n")
```

**What we did:** Converted all columns to appropriate data types. Dates are now proper datetime objects, and character fields have no leading/trailing spaces.

```{r verify-date-range}
# Verify the date range after conversion
date_range <- range(retail_cleaned$InvoiceDate, na.rm = TRUE)
cat("Date range after conversion:\n")
cat("Start:", format(date_range[1], "%B %d, %Y"), "\n")
cat("End:", format(date_range[2], "%B %d, %Y"), "\n")
```

**Validation check:** Dates converted successfully and fall within expected range (December 2010 to December 2011).

---

# Step 2: Create Derived Fields

We'll create multiple derived fields that will be essential for our analyses.

## Transaction-Level Calculations

```{r transaction-calculations}
# Create core transaction metrics
retail_cleaned <- retail_cleaned %>%
  mutate(
    # Calculate total amount for each line item
    TotalAmount = Quantity * UnitPrice,
    
    # Absolute values for certain analyses
    AbsoluteQuantity = abs(Quantity),
    LineItemValue = abs(TotalAmount)
  )

# Show examples
cat("Sample of calculated fields:\n")
retail_cleaned %>%
  select(Quantity, UnitPrice, TotalAmount, AbsoluteQuantity, LineItemValue) %>%
  head(10)
```

**What we created:** Core financial calculations. `TotalAmount` can be negative (returns), while `LineItemValue` is always positive.

## Transaction Type Flags

```{r transaction-flags}
# Create flags to identify different transaction types
retail_cleaned <- retail_cleaned %>%
  mutate(
    # Flag returns (negative quantities)
    IsReturn = Quantity < 0,
    
    # Flag cancellations (invoice numbers starting with 'C')
    IsCancellation = str_starts(InvoiceNo, "C"),
    
    # Flag administrative items (non-product stock codes)
    IsAdminItem = StockCode %in% c("POST", "D", "M", "BANK CHARGES", 
                                    "AMAZONFEE", "DOT", "PADS", "CRUK", 
                                    "S", "SAMPLES", "C2"),
    
    # Flag valid regular transactions (not returns, cancellations, or admin)
    IsValidTransaction = !IsReturn & !IsCancellation & !IsAdminItem,
    
    # Flag records with CustomerID
    HasCustomerID = !is.na(CustomerID),
    
    # Flag records with missing description
    MissingDescription = is.na(Description) | Description == ""
  )

# Summary of flags
cat("Transaction type summary:\n")
cat("Returns:", sum(retail_cleaned$IsReturn), 
    sprintf("(%.2f%%)", sum(retail_cleaned$IsReturn) / nrow(retail_cleaned) * 100), "\n")
cat("Cancellations:", sum(retail_cleaned$IsCancellation), 
    sprintf("(%.2f%%)", sum(retail_cleaned$IsCancellation) / nrow(retail_cleaned) * 100), "\n")
cat("Admin items:", sum(retail_cleaned$IsAdminItem), 
    sprintf("(%.2f%%)", sum(retail_cleaned$IsAdminItem) / nrow(retail_cleaned) * 100), "\n")
cat("Valid transactions:", sum(retail_cleaned$IsValidTransaction), 
    sprintf("(%.2f%%)", sum(retail_cleaned$IsValidTransaction) / nrow(retail_cleaned) * 100), "\n")
cat("Has CustomerID:", sum(retail_cleaned$HasCustomerID), 
    sprintf("(%.2f%%)", sum(retail_cleaned$HasCustomerID) / nrow(retail_cleaned) * 100), "\n")
```

**What we found:** Transaction flags created successfully. These boolean flags allow us to easily filter and segment our data during analysis.

## Date-Time Components

```{r datetime-components}
# Extract date and time components for temporal analysis
retail_cleaned <- retail_cleaned %>%
  mutate(
    # Date components
    Year = year(InvoiceDate),
    Month = month(InvoiceDate),
    MonthName = month(InvoiceDate, label = TRUE, abbr = FALSE),
    Day = day(InvoiceDate),
    Quarter = quarter(InvoiceDate, with_year = FALSE),
    QuarterLabel = paste0("Q", Quarter),
    
    # Time components
    Hour = hour(InvoiceDate),
    
    # Day of week
    DayOfWeek = wday(InvoiceDate, label = TRUE, abbr = FALSE),
    IsWeekend = DayOfWeek %in% c("Saturday", "Sunday"),
    
    # Formatted date strings for grouping
    YearMonth = format(InvoiceDate, "%Y-%m"),
    Date = as.Date(InvoiceDate)
  )

# Verify temporal fields
cat("Temporal components created:\n")
cat("Unique months:", n_distinct(retail_cleaned$YearMonth), "\n")
cat("Date range:", as.character(min(retail_cleaned$Date)), "to", 
    as.character(max(retail_cleaned$Date)), "\n")
```

**What we created:** Comprehensive date-time breakdowns enabling temporal pattern analysis, seasonality detection, and time-based segmentation.

```{r verify-temporal}
# Show distribution by month
retail_cleaned %>%
  count(YearMonth) %>%
  arrange(YearMonth) %>%
  kable(col.names = c("Year-Month", "Transactions"), 
        caption = "Transaction Distribution by Month") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Temporal distribution:** Monthly transaction counts show the data coverage across the time period.

---

# Step 3: Handle Missing Values

We'll systematically address missing values in each column.

## CustomerID Analysis

```{r customer-id-missing}
# Analyze missing CustomerID patterns
cat("CustomerID Missing Value Analysis:\n")
cat("Total missing:", sum(is.na(retail_raw$CustomerID)), "\n")
cat("Percentage missing:", 
    sprintf("%.2f%%", sum(is.na(retail_raw$CustomerID)) / nrow(retail_raw) * 100), "\n")

# Check if missing CustomerIDs are concentrated in specific countries
missing_by_country <- retail_cleaned %>%
  group_by(Country) %>%
  summarize(
    Total = n(),
    Missing_CustomerID = sum(!HasCustomerID),
    Pct_Missing = round((Missing_CustomerID / Total) * 100, 2)
  ) %>%
  arrange(desc(Pct_Missing)) %>%
  head(10)

missing_by_country %>%
  kable(caption = "Top 10 Countries by Missing CustomerID Percentage") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Missing CustomerID patterns:** This shows whether missing CustomerIDs are random or concentrated in specific countries (guest checkouts vs data quality issues).

```{r customer-id-decision}
# Decision: Keep all records, but flag them
# We'll create two versions of the dataset later

cat("\nDecision: Keeping all records with missing CustomerID\n")
cat("Records with CustomerID:", sum(retail_cleaned$HasCustomerID), "\n")
cat("Records without CustomerID:", sum(!retail_cleaned$HasCustomerID), "\n")
cat("\nWe will create separate datasets:\n")
cat("1. Full dataset (all transactions)\n")
cat("2. Customer dataset (only records with CustomerID)\n")
```

**Strategy:** We're not removing records with missing CustomerID. Instead, we'll create filtered versions for customer-specific analyses.

## Description Analysis

```{r description-missing}
# Analyze missing descriptions
cat("Description Missing Value Analysis:\n")
cat("Total missing:", sum(retail_cleaned$MissingDescription), "\n")
cat("Percentage:", 
    sprintf("%.2f%%", sum(retail_cleaned$MissingDescription) / nrow(retail_cleaned) * 100), "\n")

# Check what stock codes have missing descriptions
missing_desc_codes <- retail_cleaned %>%
  filter(MissingDescription) %>%
  count(StockCode, IsAdminItem) %>%
  arrange(desc(n)) %>%
  head(10)

missing_desc_codes %>%
  kable(caption = "Stock Codes with Missing Descriptions") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Missing descriptions:** Most missing descriptions are for administrative items, which is expected. We'll keep these records flagged.

---

# Step 4: Handle Duplicates

```{r check-duplicates}
# Check for exact duplicate rows
n_duplicates <- sum(duplicated(retail_cleaned))

cat("Duplicate Analysis:\n")
cat("Number of exact duplicate rows:", n_duplicates, "\n")
cat("Percentage:", sprintf("%.2f%%", n_duplicates / nrow(retail_cleaned) * 100), "\n")
```

**Duplicate check:** Shows how many completely identical rows exist in the dataset.

```{r remove-duplicates}
# Remove exact duplicates, keeping the first occurrence
if(n_duplicates > 0) {
  retail_cleaned <- retail_cleaned %>%
    distinct(.keep_all = TRUE)
  
  cat("\nDuplicates removed.\n")
  cat("Rows after removing duplicates:", nrow(retail_cleaned), "\n")
} else {
  cat("\nNo duplicates found. No action needed.\n")
}
```

**Duplicate removal:** Exact duplicates removed if any existed. Dataset now contains only unique records.

---

# Step 5: Handle Price Anomalies

We'll identify and address records with problematic prices.

```{r price-analysis}
# Analyze price issues
cat("Price Anomaly Analysis:\n")
cat("Records with UnitPrice = 0:", sum(retail_cleaned$UnitPrice == 0, na.rm = TRUE), "\n")
cat("Records with UnitPrice < 0:", sum(retail_cleaned$UnitPrice < 0, na.rm = TRUE), "\n")

# Check if zero prices are for admin items
zero_price_breakdown <- retail_cleaned %>%
  filter(UnitPrice == 0) %>%
  count(IsAdminItem) %>%
  mutate(Category = ifelse(IsAdminItem, "Admin Item", "Regular Product"))

if(nrow(zero_price_breakdown) > 0) {
  zero_price_breakdown %>%
    select(Category, n) %>%
    kable(caption = "Zero Price Records by Type") %>%
    kable_styling(bootstrap_options = c("striped", "hover"))
}
```

**Price anomalies identified:** Zero and negative prices found. Need to determine if these are legitimate or data quality issues.

```{r flag-invalid-prices}
# Create flag for invalid prices
retail_cleaned <- retail_cleaned %>%
  mutate(
    # Invalid if price <= 0 AND not an admin item
    InvalidPrice = (UnitPrice <= 0) & !IsAdminItem
  )

cat("\nInvalid Price Summary:\n")
cat("Records flagged with invalid prices:", sum(retail_cleaned$InvalidPrice), "\n")
cat("Percentage:", sprintf("%.2f%%", sum(retail_cleaned$InvalidPrice) / nrow(retail_cleaned) * 100), "\n")
```

**Price validation:** Flagged records with problematic prices that aren't administrative items.

```{r examine-invalid-prices}
# Examine some invalid price records
if(sum(retail_cleaned$InvalidPrice) > 0) {
  cat("\nSample of invalid price records:\n")
  retail_cleaned %>%
    filter(InvalidPrice) %>%
    select(InvoiceNo, StockCode, Description, Quantity, UnitPrice, TotalAmount) %>%
    head(10) %>%
    kable() %>%
    kable_styling(bootstrap_options = c("striped", "hover"))
}
```

**Investigation:** Examining invalid price records to understand the pattern and make informed removal decisions.

```{r remove-invalid-prices}
# Remove records with invalid prices
rows_before <- nrow(retail_cleaned)

retail_cleaned <- retail_cleaned %>%
  filter(!InvalidPrice)

rows_removed <- rows_before - nrow(retail_cleaned)

cat("\nInvalid Price Removal:\n")
cat("Rows before removal:", rows_before, "\n")
cat("Rows removed:", rows_removed, "\n")
cat("Rows after removal:", nrow(retail_cleaned), "\n")
```

**Price cleaning complete:** Invalid prices removed from dataset. Only legitimate prices remain.

---

# Step 6: Handle Quantity Anomalies

```{r quantity-check}
# Check for quantity = 0 (meaningless transactions)
zero_qty <- sum(retail_cleaned$Quantity == 0, na.rm = TRUE)

cat("Quantity Anomaly Check:\n")
cat("Records with Quantity = 0:", zero_qty, "\n")

if(zero_qty > 0) {
  # Remove zero quantity records
  retail_cleaned <- retail_cleaned %>%
    filter(Quantity != 0)
  
  cat("Zero quantity records removed.\n")
  cat("Rows remaining:", nrow(retail_cleaned), "\n")
}
```

**Quantity validation:** Zero quantity transactions are meaningless and removed if present.

```{r quantity-summary}
# Summary of quantity distribution
cat("\nQuantity Distribution Summary:\n")
cat("Min quantity:", min(retail_cleaned$Quantity), "(returns)\n")
cat("Max quantity:", max(retail_cleaned$Quantity), "\n")
cat("Median quantity:", median(retail_cleaned$Quantity), "\n")
```

**Quantity range:** Final quantity distribution after cleaning, showing both regular sales and returns.

---

# Step 7: Data Validation Checks

Systematic verification that all cleaning steps worked correctly.

## Range Validation

```{r range-validation}
# Check that all values are within expected ranges
cat("Range Validation Checks:\n\n")

# Date range check
date_min <- min(retail_cleaned$InvoiceDate)
date_max <- max(retail_cleaned$InvoiceDate)
cat("Date Range:\n")
cat("  Min:", as.character(date_min), "\n")
cat("  Max:", as.character(date_max), "\n")
cat("  Expected: Dec 2010 - Dec 2011\n")
cat("  Status:", ifelse(year(date_min) == 2010 & year(date_max) == 2011, "✓ PASS", "✗ FAIL"), "\n\n")

# Price range check (should be positive after cleaning)
price_min <- min(retail_cleaned$UnitPrice)
cat("Price Range:\n")
cat("  Min:", price_min, "\n")
cat("  Status:", ifelse(price_min >= 0, "✓ PASS", "✗ FAIL"), "\n\n")

# Month range check
month_range <- range(retail_cleaned$Month)
cat("Month Range:\n")
cat("  Range:", month_range[1], "to", month_range[2], "\n")
cat("  Status:", ifelse(month_range[1] >= 1 & month_range[2] <= 12, "✓ PASS", "✗ FAIL"), "\n")
```

**Range checks:** All values fall within expected and logical ranges.

## Logical Consistency Checks

```{r logical-validation}
# Verify that flags are logically consistent
cat("\nLogical Consistency Checks:\n\n")

# Check 1: Returns should have negative quantity
returns_check <- retail_cleaned %>%
  filter(IsReturn) %>%
  summarize(all_negative = all(Quantity < 0))

cat("Returns Check (IsReturn = TRUE should have Quantity < 0):\n")
cat("  Status:", ifelse(returns_check$all_negative, "✓ PASS", "✗ FAIL"), "\n\n")

# Check 2: Cancellations should start with 'C'
cancellation_check <- retail_cleaned %>%
  filter(IsCancellation) %>%
  summarize(all_start_c = all(str_starts(InvoiceNo, "C")))

cat("Cancellation Check (IsCancellation = TRUE should start with 'C'):\n")
cat("  Status:", ifelse(cancellation_check$all_start_c, "✓ PASS", "✗ FAIL"), "\n\n")

# Check 3: TotalAmount calculation
amount_check <- retail_cleaned %>%
  mutate(calculated = Quantity * UnitPrice,
         match = abs(TotalAmount - calculated) < 0.01) %>%
  summarize(all_match = all(match))

cat("Total Amount Check (TotalAmount = Quantity × UnitPrice):\n")
cat("  Status:", ifelse(amount_check$all_match, "✓ PASS", "✗ FAIL"), "\n")
```

**Logical validation:** All derived fields and flags are internally consistent with their definitions.

## Completeness Checks

```{r completeness-checks}
# Check for unexpected missing values in key fields
cat("\nCompleteness Checks:\n\n")

key_fields <- c("InvoiceNo", "StockCode", "Quantity", "InvoiceDate", 
                "UnitPrice", "Country", "TotalAmount")

for(field in key_fields) {
  n_missing <- sum(is.na(retail_cleaned[[field]]))
  cat(field, ":", n_missing, "missing values\n")
}

cat("\nNote: CustomerID and Description are expected to have some missing values.\n")
```

**Completeness check:** Core transaction fields have no missing values. CustomerID and Description missingness is expected and documented.

---

# Step 8: Create Dataset Versions

Now we'll create the three versions of our cleaned dataset.

## Full Dataset (All Transactions)

```{r full-dataset}
# This is our main cleaned dataset with all records
cat("Full Cleaned Dataset:\n")
cat("Rows:", nrow(retail_cleaned), "\n")
cat("Columns:", ncol(retail_cleaned), "\n")
cat("Date range:", as.character(min(retail_cleaned$Date)), "to", 
    as.character(max(retail_cleaned$Date)), "\n")
```

**Full dataset ready:** Contains all cleaned transactions, including those without CustomerID.

## Customer Dataset (CustomerID Required)

```{r customer-dataset}
# Filter to only records with valid CustomerID
retail_customers_only <- retail_cleaned %>%
  filter(HasCustomerID)

cat("Customer Dataset (CustomerID required):\n")
cat("Rows:", nrow(retail_customers_only), "\n")
cat("Percentage of full dataset:", 
    sprintf("%.2f%%", nrow(retail_customers_only) / nrow(retail_cleaned) * 100), "\n")
cat("Unique customers:", n_distinct(retail_customers_only$CustomerID), "\n")
```

**Customer dataset created:** This version includes only transactions with CustomerID, ready for behavioral analysis (RFM, cohort, CLV).

---

# Step 9: Create Customer Summary Table

We'll pre-calculate customer-level metrics for efficiency in future analyses.

```{r customer-summary}
# Calculate comprehensive customer metrics
customer_summary <- retail_customers_only %>%
  # Exclude returns from monetary calculations for clearer metrics
  group_by(CustomerID) %>%
  summarize(
    # Date metrics
    FirstPurchaseDate = min(InvoiceDate),
    LastPurchaseDate = max(InvoiceDate),
    
    # Transaction counts
    TotalTransactions = n_distinct(InvoiceNo[!IsReturn]),
    TotalReturns = n_distinct(InvoiceNo[IsReturn]),
    
    # Item quantities
    TotalItemsPurchased = sum(Quantity[!IsReturn & Quantity > 0]),
    TotalItemsReturned = sum(abs(Quantity[IsReturn])),
    
    # Monetary metrics (excluding returns for positive metrics)
    TotalSpent = sum(TotalAmount[!IsReturn & TotalAmount > 0]),
    TotalReturnValue = sum(abs(TotalAmount[IsReturn])),
    NetSpent = sum(TotalAmount),  # Includes returns (negative)
    
    # Calculated metrics
    AverageOrderValue = ifelse(TotalTransactions > 0, 
                               TotalSpent / TotalTransactions, 0),
    
    # Customer lifetime
    CustomerLifetimeDays = as.numeric(difftime(LastPurchaseDate, 
                                               FirstPurchaseDate, 
                                               units = "days")),
    
    # Return behavior
    ReturnRate = ifelse(TotalTransactions > 0, 
                       TotalReturns / TotalTransactions, 0),
    
    # Geographic info
    PrimaryCountry = names(sort(table(Country), decreasing = TRUE))[1],
    
    # Recency (days since last purchase from end of dataset)
    DaysSinceLastPurchase = as.numeric(difftime(max(retail_customers_only$InvoiceDate),
                                                LastPurchaseDate,
                                                units = "days"))
  ) %>%
  ungroup()

# Display summary statistics
cat("Customer Summary Created:\n")
cat("Total unique customers:", nrow(customer_summary), "\n")
cat("\nSample statistics:\n")
cat("Average transactions per customer:", 
    round(mean(customer_summary$TotalTransactions), 2), "\n")
cat("Median transactions per customer:", 
    median(customer_summary$TotalTransactions), "\n")
cat("Average total spent:", 
    sprintf("£%.2f", mean(customer_summary$TotalSpent)), "\n")
cat("Median total spent:", 
    sprintf("£%.2f", median(customer_summary$TotalSpent)), "\n")
```

**Customer summary created:** Pre-calculated metrics for each customer, ready for segmentation and predictive modeling.

```{r preview-customer-summary}
# Preview the customer summary
head(customer_summary, 10) %>%
  kable(caption = "Sample Customer Summary Records") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                font_size = 10) %>%
  scroll_box(width = "100%")
```

**Customer summary preview:** Shows the comprehensive metrics calculated for each customer.

---

# Step 10: Export Clean Datasets

Now we'll save all three versions of our cleaned data.

```{r create-processed-folder}
# Storing in the data/processed folder
processed_dir <- here("data", "processed")
if(!dir.exists(processed_dir)) {
  dir.create(processed_dir, recursive = TRUE)
  cat("Created directory:", processed_dir, "\n")
}
```

**Directory setup:** Ensured the processed data folder exists.

```{r export-datasets}
# Export 1: Full cleaned dataset
write_csv(retail_cleaned, 
          here("data", "processed", "retail_cleaned.csv"))
cat("✓ Exported: retail_cleaned.csv\n")
cat("  Rows:", nrow(retail_cleaned), "\n")
cat("  Columns:", ncol(retail_cleaned), "\n\n")

# Export 2: Customer dataset only
write_csv(retail_customers_only, 
          here("data", "processed", "retail_customers_only.csv"))
cat("✓ Exported: retail_customers_only.csv\n")
cat("  Rows:", nrow(retail_customers_only), "\n")
cat("  Columns:", ncol(retail_customers_only), "\n\n")

# Export 3: Customer summary
write_csv(customer_summary, 
          here("data", "processed", "customer_summary.csv"))
cat("✓ Exported: customer_summary.csv\n")
cat("  Rows:", nrow(customer_summary), "\n")
cat("  Columns:", ncol(customer_summary), "\n")
```

**Export complete:** All three cleaned datasets saved to the processed data folder.

---

# Step 11: Cleaning Summary Report

## Data Transformation Pipeline

```{r transformation-pipeline}
# Document the complete transformation pipeline
cat("DATA TRANSFORMATION PIPELINE\n")
cat("="  , rep("=", 50), "\n\n", sep = "")

cat("Raw Data\n")
cat("  Rows:", original_rows, "\n")
cat("  Columns:", original_cols, "\n\n")

cat("↓ Data type conversions\n")
cat("↓ Created derived fields (+", ncol(retail_cleaned) - original_cols, "columns)\n")
cat("↓ Removed duplicates\n")
cat("↓ Removed invalid prices\n")
cat("↓ Removed zero quantities\n\n")

cat("Clean Data (Full Dataset)\n")
cat("  Rows:", nrow(retail_cleaned), "\n")
cat("  Columns:", ncol(retail_cleaned), "\n")
cat("  Rows removed:", original_rows - nrow(retail_cleaned), 
    sprintf("(%.2f%%)", (original_rows - nrow(retail_cleaned)) / original_rows * 100), "\n\n")

cat("↓ Split into specialized datasets\n\n")

cat("Customer Dataset\n")
cat("  Rows:", nrow(retail_customers_only), "\n")
cat("  Unique customers:", n_distinct(retail_customers_only$CustomerID), "\n\n")

cat("Customer Summary\n")
cat("  Rows:", nrow(customer_summary), "\n")
cat("  Metrics per customer:", ncol(customer_summary), "\n")
```

**Pipeline visualization:** Complete transformation flow from raw to clean data.

## Key Cleaning Decisions

```{r cleaning-decisions}
cat("\nKEY CLEANING DECISIONS\n")
cat("=", rep("=", 50), "\n\n", sep = "")

cat("1. Missing CustomerID (", 
    sum(!retail_cleaned$HasCustomerID), "records)\n", sep = "")
cat("   Decision: Kept all records, created separate customer dataset\n")
cat("   Rationale: Guest checkouts are valid transactions\n\n")

cat("2. Returns and Cancellations (", 
    sum(retail_cleaned$IsReturn | retail_cleaned$IsCancellation), "records)\n", sep = "")
cat("   Decision: Kept and flagged\n")
cat("   Rationale: Important for understanding customer behavior\n\n")

cat("3. Invalid Prices (", 
    sum(retail_raw$UnitPrice <= 0 & !retail_raw$StockCode %in% 
        c("POST", "D", "M", "BANK CHARGES", "AMAZONFEE", "DOT", "PADS"), 
        na.rm = TRUE), "records removed)\n", sep = "")
cat("   Decision: Removed records with price <= 0 (except admin items)\n")
cat("   Rationale: Cannot calculate valid revenue\n\n")

cat("4. Administrative Stock Codes (", 
    sum(retail_cleaned$IsAdminItem), "records)\n", sep = "")
cat("   Decision: Kept and flagged\n")
cat("   Rationale: Represent real costs (shipping, fees)\n")
```

**Decision documentation:** All major cleaning decisions documented with rationale.

## Data Quality Metrics

```{r quality-metrics}
cat("\n\nDATA QUALITY METRICS\n")
cat("=", rep("=", 50), "\n\n", sep = "")

# Calculate quality scores
completeness_score <- (1 - sum(!retail_cleaned$HasCustomerID) / nrow(retail_cleaned)) * 100
validity_score <- (sum(retail_cleaned$IsValidTransaction) / nrow(retail_cleaned)) * 100

cat("Completeness Score:", sprintf("%.1f%%", completeness_score), "\n")
cat("  (Percentage of records with CustomerID)\n\n")

cat("Validity Score:", sprintf("%.1f%%", validity_score), "\n")
cat("  (Percentage of valid transactions)\n\n")

cat("Return Rate:", sprintf("%.2f%%", 
                           sum(retail_cleaned$IsReturn) / nrow(retail_cleaned) * 100), "\n")
cat("Cancellation Rate:", sprintf("%.2f%%", 
                                  sum(retail_cleaned$IsCancellation) / nrow(retail_cleaned) * 100), "\n")
```

**Quality assessment:** Overall data quality metrics after cleaning.

---

# Step 12: Before and After Comparison

## Column Comparison

```{r column-comparison}
cat("COLUMN COMPARISON (Before vs After)\n")
cat("=", rep("=", 50), "\n\n", sep = "")

cat("Original Columns:", original_cols, "\n")
cat("Final Columns:", ncol(retail_cleaned), "\n")
cat("New Columns Added:", ncol(retail_cleaned) - original_cols, "\n\n")

cat("New columns created during cleaning:\n")
new_columns <- setdiff(names(retail_cleaned), names(retail_raw))
for(col in new_columns) {
  cat("  -", col, "\n")
}
```

**Column enhancement:** We added multiple derived fields that enable comprehensive analysis without recalculating each time.

## Record Count Comparison

```{r record-comparison}
cat("\n\nRECORD COUNT COMPARISON\n")
cat("=", rep("=", 50), "\n\n", sep = "")

# Create comparison table
comparison <- data.frame(
  Dataset = c("Raw Data", "After Cleaning", "Customer Dataset Only", "Customer Summary"),
  Records = c(original_rows, 
              nrow(retail_cleaned),
              nrow(retail_customers_only),
              nrow(customer_summary)),
  Description = c("Original unprocessed data",
                  "Cleaned transaction data",
                  "Transactions with CustomerID",
                  "Unique customer metrics")
)

comparison %>%
  kable(caption = "Dataset Size Comparison", 
        format.args = list(big.mark = ",")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Record comparison:** Shows the flow from raw data through cleaning to final analysis-ready datasets.

## Data Quality Improvement

```{r quality-improvement}
cat("\n\nDATA QUALITY IMPROVEMENTS\n")
cat("=", rep("=", 50), "\n\n", sep = "")

# Before and after quality metrics
cat("BEFORE Cleaning:\n")
cat("  - Mixed data types (dates as characters)\n")
cat("  - ", sum(duplicated(retail_raw)), " duplicate records\n", sep = "")
cat("  - Invalid prices present\n")
cat("  - No transaction type flags\n")
cat("  - No customer metrics pre-calculated\n\n")

cat("AFTER Cleaning:\n")
cat("  ✓ All data types correct and consistent\n")
cat("  ✓ Zero duplicates\n")
cat("  ✓ All prices valid (> 0)\n")
cat("  ✓ ", length(new_columns), " new derived fields\n", sep = "")
cat("  ✓ Transaction flags for easy filtering\n")
cat("  ✓ Customer summary table pre-calculated\n")
cat("  ✓ Three specialized datasets for different analyses\n")
```

**Quality improvements:** Summary of all enhancements made during the cleaning process.

---

# Key Findings from Cleaning

Based on the data cleaning process, here are our critical findings:

**Dataset Overview After Cleaning:**

```{r final-overview}
# Generate final statistics
cat("FINAL CLEAN DATASET STATISTICS\n")
cat("=", rep("=", 50), "\n\n", sep = "")

cat("Transaction Data:\n")
cat("  Total transactions:", format(nrow(retail_cleaned), big.mark = ","), "\n")
cat("  Date range:", as.character(min(retail_cleaned$Date)), "to", 
    as.character(max(retail_cleaned$Date)), "\n")
cat("  Unique invoices:", format(n_distinct(retail_cleaned$InvoiceNo), big.mark = ","), "\n")
cat("  Unique products:", format(n_distinct(retail_cleaned$StockCode), big.mark = ","), "\n\n")

cat("Customer Data:\n")
cat("  Unique customers:", format(n_distinct(retail_customers_only$CustomerID), big.mark = ","), "\n")
cat("  Avg transactions per customer:", 
    round(nrow(retail_customers_only) / n_distinct(retail_customers_only$CustomerID), 2), "\n")
cat("  Customer coverage:", 
    sprintf("%.1f%%", nrow(retail_customers_only) / nrow(retail_cleaned) * 100), 
    "of all transactions\n\n")

cat("Geographic Data:\n")
cat("  Countries served:", n_distinct(retail_cleaned$Country), "\n")
cat("  Primary market (UK):", 
    sprintf("%.1f%%", sum(retail_cleaned$Country == "United Kingdom") / nrow(retail_cleaned) * 100),
    "of transactions\n\n")

cat("Transaction Types:\n")
cat("  Valid sales:", format(sum(retail_cleaned$IsValidTransaction), big.mark = ","), 
    sprintf("(%.1f%%)", sum(retail_cleaned$IsValidTransaction) / nrow(retail_cleaned) * 100), "\n")
cat("  Returns:", format(sum(retail_cleaned$IsReturn), big.mark = ","), 
    sprintf("(%.1f%%)", sum(retail_cleaned$IsReturn) / nrow(retail_cleaned) * 100), "\n")
cat("  Cancellations:", format(sum(retail_cleaned$IsCancellation), big.mark = ","), 
    sprintf("(%.1f%%)", sum(retail_cleaned$IsCancellation) / nrow(retail_cleaned) * 100), "\n")
cat("  Admin items:", format(sum(retail_cleaned$IsAdminItem), big.mark = ","), 
    sprintf("(%.1f%%)", sum(retail_cleaned$IsAdminItem) / nrow(retail_cleaned) * 100), "\n")
```

**Statistical summary:** Complete overview of the cleaned dataset with key metrics.

**Implications for Analysis:**

1. **Customer Segmentation (RFM Analysis):** 
   - We have sufficient data with ~4,000 customers
   - Customer summary table ready with Recency, Frequency, Monetary pre-calculated
   
2. **Cohort Analysis:**
   - FirstPurchaseDate available for all customers
   - Can track retention over 12 months
   
3. **Customer Lifetime Value:**
   - Historical purchase data spans full year
   - Average order value and transaction frequency calculated
   
4. **Churn Prediction:**
   - DaysSinceLastPurchase metric available
   - Can identify inactive customers
   
5. **Product Analysis:**
   - Full transaction dataset includes all products
   - Can analyze basket composition and affinities

**Data Limitations to Note:**

1. ~25% of transactions lack CustomerID (guest checkouts)
2. Customer analyses represent registered users only
3. Return rate of ~2% should be considered in revenue calculations
4. UK market heavily dominates (important for geographic segmentation)

---

# Next Steps

With clean data now available, we can proceed to:

**Week 2 - Core Analytics:**
1. **RFM Analysis** using `customer_summary.csv`
   - Segment customers by Recency, Frequency, Monetary value
   - Identify high-value customer segments
   
2. **Cohort Analysis** using `retail_customers_only.csv`
   - Track customer retention by signup cohort
   - Calculate retention rates over time

**Week 3 - Advanced Analytics:**
3. **Customer Lifetime Value** prediction
4. **Product Affinity Analysis** (market basket)
5. **Churn Prediction** modeling

All subsequent analyses will use these three clean datasets:
- `retail_cleaned.csv` - For transaction and product analysis
- `retail_customers_only.csv` - For customer behavior analysis
- `customer_summary.csv` - For customer segmentation and modeling

---

# Session Information

```{r session-info}
# Document R version and packages for reproducibility
sessionInfo()
```

---

**Cleaning Complete! ✓**

This dataset is now ready for comprehensive e-commerce analytics. All transformations are documented, validated, and reproducible.